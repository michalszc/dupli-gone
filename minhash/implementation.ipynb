{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e504f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ef4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xml_from_url(url: str) -> pd.DataFrame:\n",
    "    r = requests.get(url)\n",
    "\n",
    "    root = ET.fromstring(r.text)\n",
    "    records = root.find('records') \n",
    "\n",
    "    def extract(child):\n",
    "        parts = []\n",
    "        for sub in child:\n",
    "            if len(sub) != 0:\n",
    "                parts.append(extract(sub))\n",
    "                continue\n",
    "\n",
    "            if sub.tag.lower() in ('_face','_font','_size'):\n",
    "                continue\n",
    "\n",
    "            if sub.text and sub.text.strip():\n",
    "                parts.append(sub.text.strip())\n",
    "        text = ','.join(parts)\n",
    "\n",
    "        return text\n",
    "\n",
    "    rows = []\n",
    "    for rec in records.findall('record'):\n",
    "        row = {}\n",
    "        for child in rec:\n",
    "            text = ''\n",
    "            tag = child.tag\n",
    "            if len(child) == 0:\n",
    "                text = (child.text or '').strip()\n",
    "            else:\n",
    "                text = extract(child)\n",
    "\n",
    "            row[tag] = text\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fadc47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Benchmark:\n",
    "    '''\n",
    "    A class for benchmarking classification models.\n",
    "\n",
    "    Args:\n",
    "        model (Any): A model object that has a `predict` method.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model: Any, average: str = 'binary'):\n",
    "        self.model = model\n",
    "        self.average = average\n",
    "\n",
    "    def evaluate(self, X, y_true) -> Dict[str, float]:\n",
    "        '''\n",
    "        Calculates classification metrics and prediction time.\n",
    "\n",
    "        Args:\n",
    "            X: Input features.\n",
    "            y_true: True labels.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing metrics and prediction time.\n",
    "        '''\n",
    "        start_time = time.time()\n",
    "        y_pred = self.model.predict(X)\n",
    "        end_time = time.time()\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average='binary', zero_division=0),\n",
    "            'recall': recall_score(y_true, y_pred, average='binary', zero_division=0),\n",
    "            'f1': f1_score(y_true, y_pred, average='binary', zero_division=0),\n",
    "            'prediction_time_sec': end_time - start_time,\n",
    "            'number_of_samples': len(y_true),\n",
    "        }\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7cfbd052",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinHashDeduplicator:\n",
    "    '''\n",
    "    A class to deduplicate articles using MinHash\n",
    "    '''\n",
    "    def __init__(self, k=10, treshold=0.51):\n",
    "        self.k = k\n",
    "        self.treshold = treshold\n",
    "\n",
    "    def predict(self, articles: pd.DataFrame) -> List[int]:\n",
    "        # change each row to set without hashed values, without NaNs\n",
    "        rows = []\n",
    "        for _, row in articles.iterrows():\n",
    "            row = row.dropna().apply(hash).to_list()\n",
    "            row.sort()\n",
    "            rows.append(set(row[:self.k]))\n",
    "\n",
    "        result = [0 for _ in rows]\n",
    "        # for each pair of rows calculate estimate\n",
    "        for i in range(len(result)):\n",
    "            for j in range(1, len(result)):\n",
    "                X = set(sorted(rows[i] | rows[j])[:self.k])\n",
    "                Y = X & rows[i] & rows[j]\n",
    "                if len(Y) / self.k > self.treshold:\n",
    "                    result[i] = 1\n",
    "                    result[j] = 1\n",
    "\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "579bce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tafenoquine: {'accuracy': 0.6480446927374302, 'precision': 0.6440677966101694, 'recall': 1.0, 'f1': 0.7835051546391752, 'prediction_time_sec': 0.07968568801879883, 'number_of_samples': 179}\n",
      "uti: {'accuracy': 0.4429530201342282, 'precision': 0.40523560209424087, 'recall': 0.9675, 'f1': 0.5712177121771218, 'prediction_time_sec': 2.1759092807769775, 'number_of_samples': 1043}\n",
      "diabetes: {'accuracy': 0.5066334991708126, 'precision': 0.4905379228132916, 'recall': 0.9561428986349114, 'f1': 0.6484144179633642, 'prediction_time_sec': 107.7034056186676, 'number_of_samples': 7236}\n",
      "copper: {'accuracy': 0.5544554455445545, 'precision': 0.5557768924302788, 'recall': 0.9928825622775801, 'f1': 0.7126436781609196, 'prediction_time_sec': 0.5865988731384277, 'number_of_samples': 505}\n",
      "blue-light: {'accuracy': 0.5573394495412844, 'precision': 0.5565217391304348, 'recall': 0.939203354297694, 'f1': 0.6989079563182528, 'prediction_time_sec': 1.6579370498657227, 'number_of_samples': 872}\n"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    \"tafenoquine\",\n",
    "    \"uti\",\n",
    "    \"diabetes\",\n",
    "    \"copper\",\n",
    "    \"blue-light\",\n",
    "]\n",
    "\n",
    "deduplicator = MinHashDeduplicator(treshold=0.80)\n",
    "benchmark = Benchmark(deduplicator)\n",
    "\n",
    "for dataset in datasets:\n",
    "    url = \"https://raw.githubusercontent.com/IEBH/dedupe-sweep/master/test/data/\" + dataset + \".xml\"\n",
    "    df = load_xml_from_url(url)\n",
    "    df['label'] = df[\"caption\"].apply(lambda x: 1 if x == \"Duplicate\" else 0)\n",
    "    print(f\"{dataset}: {benchmark.evaluate(df.drop(columns=[\"caption\", \"label\"]), df[\"label\"])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
